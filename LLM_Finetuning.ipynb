{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmHGlefHXR7V0L0jRe3ThL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Azlan-Qaisrani/my-first/blob/main/LLM_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOEV1_kTHX4e"
      },
      "outputs": [],
      "source": [
        "# Install and import MIT Deep Learning utilities\n",
        "!pip install mitdeeplearning > /dev/null 2>&1\n",
        "import mitdeeplearning as mdl\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from lion_pytorch import Lion"
      ],
      "metadata": {
        "id": "k9WrDL3MHw7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Basic question-answer template\n",
        "template_without_answer = \"<start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "template_with_answer = template_without_answer + \"{answer}<end_of_turn>\\n\"\n",
        "\n",
        "print(template_with_answer.format(question=\"What is your name?\", answer=\"My name is Gemma!\"))\n"
      ],
      "metadata": {
        "id": "LE286Rp-InKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"unsloth/gemma-2-2b-it\" #\"google/gemma-2-2b-it\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "print(f\"Tokenizer vocab size: {len(tokenizer.get_vocab())}\")"
      ],
      "metadata": {
        "id": "2MqvtrDrLm89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Here is some sample text\"\n",
        "print(f\"Orignal text:{text}\")\n",
        "\n",
        "tokens=tokenizer.encode(text,return_tensors=\"pt\")\n",
        "print(f\"Encoded tokens: {tokens}\")\n",
        "decoded_text=tokenizer.decode(tokens[0],skip_special_tokens=True)\n",
        "print(f\"Decoded text: {decoded_text}\")"
      ],
      "metadata": {
        "id": "c6bTNiiwJgbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=template_without_answer.format(question=\"What is the capital of Pakistan? Use one word.\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "BSNnv-RXL-Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=AutoModelForCausalLM.from_pretrained(model_id,device_map=\"auto\")"
      ],
      "metadata": {
        "id": "TKTS1I5uMldg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"What is the captial of Pakistan?Use one word.\"\n",
        "prompt=template_without_answer.format(question=question)\n",
        "\n",
        "tokens=tokenizer.encode(prompt,return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  output=model(tokens)\n",
        "  probs=F.softmax(output.logits,dim=-1)\n",
        "\n",
        "\n",
        "next_token=torch.argmax(probs[0,-1,:]).to(model.device)\n",
        "next_token=next_token.item()\n",
        "\n",
        "next_token_text=tokenizer.decode(next_token)\n",
        "print(prompt)\n",
        "print(next_token_text)"
      ],
      "metadata": {
        "id": "bIP9qmxRNf0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=template_without_answer.format(question=\"What does NASA stands for?\")\n",
        "tokens=tokenizer.encode(prompt,return_tensors=\"pt\").to(model.device)\n",
        "output=model.generate(tokens,max_new_tokens=20)\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "FiECQVPmQfoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader,test_loader=mdl.lab3.create_dataloader(style=\"leprechaun\")\n",
        "\n",
        "sample=train_loader.dataset[44]\n",
        "question=sample[\"instruction\"]\n",
        "answer=sample[\"response\"]\n",
        "answer_style=sample[\"response_style\"]\n",
        "\n",
        "print(f\"Question: {question}\\n\\n\"+\n",
        "      f\"Orignal Answer: {answer}\\n\\n\"+\n",
        "      f\"Answer Style: {answer_style}\")"
      ],
      "metadata": {
        "id": "njl5_P_6SO4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(question, max_new_tokens=32, temperature=0.7, only_answer=False):\n",
        "    prompt = template_without_answer.format(question=question)\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**input_ids, do_sample=True, max_new_tokens=max_new_tokens, temperature=temperature)\n",
        "\n",
        "\n",
        "\n",
        "    output_tokens = outputs[0]\n",
        "    if only_answer:\n",
        "        output_tokens = output_tokens[input_ids['input_ids'].shape[1]:]\n",
        "\n",
        "\n",
        "    result = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "AKY6yg7KUBcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = chat(\n",
        "    \"deep learning?\",\n",
        "    only_answer=True,\n",
        "    max_new_tokens=500,\n",
        "    temperature=1\n",
        ")\n",
        "\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "Wvu3IGVLVwhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_lora(model):\n",
        "  lora_config=LoraConfig(\n",
        "      r=8,\n",
        "      task_type=\"CAUSAL_LM\",\n",
        "      target_modules=[\n",
        "            \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "        ],\n",
        "  )\n",
        "  lora_model=get_peft_model(model,lora_config)\n",
        "  return lora_model\n",
        "model=apply_lora(model)\n",
        "trainable_params=sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params=sum(p.numel() for p in model.parameters())\n",
        "print(f\"number of trainable parameters: {trainable_params}\")\n",
        "print(f\"total parameters: {total_params}\")\n",
        "print(f\"percentage of trainable parameters: {trainable_params / total_params * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "3mAmFJRvZaqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_and_compute_loss(model,tokens,mask,context_length=512):\n",
        "  tokens=tokens[:, :context_length]\n",
        "  mask=mask[:, :context_length]\n",
        "\n",
        "  x=tokens[:, :-1]\n",
        "  y=tokens[:, 1:]\n",
        "  mask=mask[:,1:]\n",
        "\n",
        "  logits=model(x).logits\n",
        "\n",
        "  loss=F.cross_entropy(\n",
        "      logits.view(-1,logits.size(-1)),\n",
        "      y.view(-1),\n",
        "      reduction=\"none\"\n",
        "  )\n",
        "  loss=loss[mask.view(-1)].mean()\n",
        "\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "d7m3uTn5bGuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,dataloader,tokenizer,max_steps=100,context_length=512,learning_rate=1e-4):\n",
        "  losses=[]\n",
        "  model=apply_lora(model)\n",
        "\n",
        "  optimizer=Lion(model.parameters(),lr=learning_rate)\n",
        "\n",
        "  for step,batch in enumerate(dataloader):\n",
        "\n",
        "    question=batch[\"instruction\"][0]\n",
        "    answer=batch[\"response_style\"][0]\n",
        "\n",
        "    text=template_with_answer.format(question=question,answer=answer)\n",
        "\n",
        "    ids=tokenizer(text,return_tensors=\"pt\",return_offsets_mapping=True).to(model.device)\n",
        "    mask = ids[\"offset_mapping\"][:,:,0] >= text.index(answer)\n",
        "\n",
        "\n",
        "    loss=forward_and_compute_loss(\n",
        "        model=model,\n",
        "        tokens=ids[\"input_ids\"],\n",
        "        mask=mask,\n",
        "        context_length=context_length,\n",
        "    )\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    if step % 10 == 0:\n",
        "            print(chat(\"What is the capital of France?\", only_answer=True))\n",
        "            print(f\"step {step} loss: {torch.mean(torch.tensor(losses)).item()}\")\n",
        "            losses = []\n",
        "\n",
        "    if step > 0 and step % max_steps == 0:\n",
        "            break\n",
        "\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "eOqPjXXrf--p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moedl=train(model,train_loader,tokenizer,max_steps=50)"
      ],
      "metadata": {
        "id": "A2FEJuQJktOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat(\"What is a good story about tennis\", only_answer=True, max_new_tokens=200))\n"
      ],
      "metadata": {
        "id": "aGxZQX_zk-Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader,test_loader=mdl.lab3.create_dataloader(style=\"yoda\")\n",
        "model=train(model,train_loader,tokenizer,max_steps=50)"
      ],
      "metadata": {
        "id": "A_izuA4XmbmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are an impartial judge that evaluates if text was written by {style}.\n",
        "\n",
        "An example piece of text from {style} is:\n",
        "{example}\n",
        "\n",
        "Now, analyze some new text carefully and respond on if it follows the\n",
        "same style of {style}. Be critical to identify any issues in the text.\n",
        "Then convert your feedback into a number between 0 and 10: 10 if the text\n",
        "is written exactly in the style of {style}, 5 if mixed faithfulness to the\n",
        "style, or 0 if the text is not at all written in the style of {style}.\n",
        "\n",
        "The format of the your response should be a JSON dictionary and nothing else:\n",
        "{{\"score\": <score between 0 and 10>}}\n",
        "\"\"\"\n",
        "style=\"Yoda\"\n",
        "example = \"The very Republic is threatened, if involved the Sith are. Hard to see, the dark side is. Discover who this assassin is, we must. With this Naboo queen you must stay, Qui-Gon. Protect her. May the Force be with you. A vergence, you say? But you do! Revealed your opinion is. Trained as a Jedi, you request for him? Good, good, young one.\"\n",
        "system_prompt=system_prompt.format(style=style,example=example)\n",
        "print(\"=== System prompt ===\")\n",
        "print(system_prompt)\n"
      ],
      "metadata": {
        "id": "T4BX9qD9riuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENROUTER_API_KEY = \"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\" # TODO: add your OpenRouter API key here\n",
        "assert OPENROUTER_API_KEY != \"\", \"You must set your OpenRouter API key before running this cell!\"\n",
        "\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "llm = mdl.lab3.LLMClient(model=model_name, api_key=OPENROUTER_API_KEY)"
      ],
      "metadata": {
        "id": "WibfvQBer44K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from opik.evaluation.metrics import base_metric, score_result\n",
        "\n",
        "class LLMJudgeEvaluator(base_metric.BaseMetric):\n",
        "    def __init__(self, judge: mdl.lab3.LLMClient = None, system_prompt: str = None):\n",
        "        self.judge = judge\n",
        "        self.system_prompt = system_prompt\n",
        "        self.prompt_template = \"Evaluate this text: {text}\"\n",
        "\n",
        "    def score(self, text: str, n_tries=20, **kwargs):\n",
        "        \"\"\" Evaluate by asking an LLM to score it. \"\"\"\n",
        "\n",
        "        for attempt in range(n_tries):\n",
        "            try:\n",
        "                prompt = self.prompt_template.format(text=text)\n",
        "\n",
        "                stop = \"}\"\n",
        "\n",
        "\n",
        "                res = self.judge.ask(\n",
        "                    system=self.system_prompt,\n",
        "                    user=prompt,\n",
        "                    max_tokens=10,\n",
        "                    stop=[stop]\n",
        "                )\n",
        "\n",
        "\n",
        "                res = res.choices[0].message.content + stop\n",
        "                res_dict = json.loads(res)\n",
        "\n",
        "                max_score = 10 # The maximum score that the LLM should output\n",
        "                score = res_dict[\"score\"] / max_score # Normalize\n",
        "                score = max(0.0, min(score, 1.0)) # Clip between 0 and 1\n",
        "\n",
        "                return score_result.ScoreResult(name=\"StyleScore\", value=score)\n",
        "\n",
        "            except Exception as e:\n",
        "                if attempt == n_tries - 1:  # Last attempt\n",
        "                    raise e  # Re-raise the exception if all attempts failed\n",
        "                continue"
      ],
      "metadata": {
        "id": "Z-lviDLXth0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "judge=LLMJudgeEvaluator(llm,system_prompt=system_prompt)"
      ],
      "metadata": {
        "id": "0GoZb0xivpKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring_fuction(text):\n",
        "  return judge.score(text).value\n",
        "test_texts=[\n",
        "    \"Tennis is a fun sport. But you must concentrate.\",\n",
        "    \"Fun sport, tennis is. But work hard, you must.\",\n",
        "    \"Hard to see, the dark side is.\"\n",
        "\n",
        "]\n",
        "for text in test_texts:\n",
        "  score=scoring_fuction(text)\n",
        "  print(f\"{text} ==> Score: {score}\")"
      ],
      "metadata": {
        "id": "5-g0355Avw5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text from your model by asking it new questions.\n",
        "def generate_samples_from_test(test_loader, num_samples):\n",
        "    samples = []\n",
        "    for test_sample in tqdm(test_loader, total=num_samples):\n",
        "        test_question = test_sample['instruction'][0]\n",
        "        with torch.no_grad():\n",
        "            generated = chat(test_question, only_answer=True, max_new_tokens=100)\n",
        "        samples.append(generated)\n",
        "        if len(samples) >= num_samples:\n",
        "            break\n",
        "    return samples\n",
        "\n",
        "n_samples = 20\n",
        "generated_samples = generate_samples_from_test(test_loader, num_samples=n_samples)\n"
      ],
      "metadata": {
        "id": "9nmbWTSowIma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_samples = [sample['response'][0] for i, sample in enumerate(train_loader) if i < n_samples]\n",
        "style_samples = [sample['response_style'][0] for i, sample in enumerate(train_loader) if i < n_samples]"
      ],
      "metadata": {
        "id": "j8fUPxvRw-2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def compute_scores_in_parallel(samples):\n",
        "    with Pool(processes=10) as pool:\n",
        "        scores = pool.map(scoring_fuction, samples)\n",
        "    return scores\n",
        "\n",
        "# Compute and print the scores for the base-style text, generated text, and training-set text in Yoda-speak style\n",
        "base_scores = compute_scores_in_parallel(base_samples)\n",
        "print(f\"Base: {np.mean(base_scores):.2f} ± {np.std(base_scores):.2f}\")\n",
        "\n",
        "generated_scores = compute_scores_in_parallel(generated_samples)\n",
        "print(f\"Gen: {np.mean(generated_scores):.2f} ± {np.std(generated_scores):.2f}\")\n",
        "\n",
        "style_scores = compute_scores_in_parallel(style_samples)\n",
        "print(f\"Train: {np.mean(style_scores):.2f} ± {np.std(style_scores):.2f}\")"
      ],
      "metadata": {
        "id": "CAavMHLcxClm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yoda_test_text = mdl.lab3.yoda_test_text\n",
        "tokens = tokenizer(yoda_test_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Get the loglikelihood from the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**tokens)\n",
        "    logits = outputs.logits[:, :-1]\n",
        "    targets = tokens.input_ids[:, 1:]\n",
        "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)),\n",
        "                            targets.reshape(-1))\n",
        "\n",
        "print(f\"Yoda test loglikelihood: {loss.item():.2f}\")"
      ],
      "metadata": {
        "id": "F3j9QdFpxPfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gOmNH4lkEcDx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}